{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchsummary\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from sh import gunzip\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "\n",
    "class FashionMnistLoader:\n",
    "    \n",
    "    dir_name = \"data/fashion\"\n",
    "    url_train_imgs = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\"\n",
    "    url_train_labels = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\"\n",
    "    url_test_imgs = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\"\n",
    "    url_test_labels = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_imgs_fn = None\n",
    "        self.train_labels_fn = None\n",
    "        self.test_imgs_fn = None\n",
    "        self.test_labels_fn = None\n",
    "        \n",
    "    def get_data(self, url):\n",
    "        gz_file_name = url.split(\"/\")[-1]\n",
    "        gz_file_path = os.path.join(self.dir_name, gz_file_name)  \n",
    "        file_name = gz_file_name.split(\".\")[0]\n",
    "        file_path = os.path.join(self.dir_name, file_name)\n",
    "        os.makedirs(self.dir_name, exist_ok=True)\n",
    "        if not os.path.exists(file_path):\n",
    "            urllib.request.urlretrieve(url, gz_file_path) \n",
    "            gunzip(gz_file_path)  \n",
    "        return file_path\n",
    "        \n",
    "    def get_all_data(self):\n",
    "        self.train_imgs_fn = self.get_data(self.url_train_imgs)\n",
    "        self.train_labels_fn = self.get_data(self.url_train_labels)\n",
    "        self.test_imgs_fn = self.get_data(self.url_test_imgs)\n",
    "        self.test_labels_fn = self.get_data(self.url_test_labels)\n",
    "        return self\n",
    "    \n",
    "    def load_train(self):\n",
    "        X, y = loadlocal_mnist(\n",
    "            images_path=self.train_imgs_fn, \n",
    "            labels_path=self.train_labels_fn)\n",
    "        return X, y\n",
    "    \n",
    "    def load_test(self):\n",
    "        X, y = loadlocal_mnist(\n",
    "            images_path=self.test_imgs_fn, \n",
    "            labels_path=self.test_labels_fn)\n",
    "        return X, y\n",
    "    \n",
    "    def _split(self, X, y, test_size):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=666)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "            \n",
    "    def train_split(self, test_size):\n",
    "        X, y = self.load_train()\n",
    "        X_train, X_test, y_train, y_test = self._split(X, y, test_size)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def standard_split(self):\n",
    "        X_train, y_train = self.load_train()\n",
    "        X_test, y_test = self.load_test()\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToPILImage(), transforms.ToTensor(), transforms.Normalize([0], [1])])\n",
    "\n",
    "class FashionMnist(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.data = (torch.from_numpy(X).float()/255).reshape(-1, 1, 28, 28).squeeze()\n",
    "        self.target = torch.from_numpy(y).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img, tar = self.data[index], self.target[index]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, tar\n",
    "    \n",
    "## Windows users\n",
    "## Ctrl + / to uncommnet\n",
    "# import os\n",
    "#import urllib.request\n",
    "#import gzip\n",
    "#import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from mlxtend.data import loadlocal_mnist\n",
    "#\n",
    "# class FashionMnistLoader:\n",
    "    \n",
    "#     dir_name = \"data/fashion\"\n",
    "#     url_train_imgs = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\"\n",
    "#     url_train_labels = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\"\n",
    "#     url_test_imgs = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\"\n",
    "#     url_test_labels = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\"\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         self.train_imgs_fn = None\n",
    "#         self.train_labels_fn = None\n",
    "#         self.test_imgs_fn = None\n",
    "#         self.test_labels_fn = None\n",
    "        \n",
    "#     def get_data(self, url):\n",
    "#         gz_file_name = url.split(\"/\")[-1]\n",
    "#         gz_file_path = os.path.join(self.dir_name, gz_file_name)  \n",
    "#         file_name = gz_file_name.split(\".\")[0]\n",
    "#         file_path = os.path.join(self.dir_name, file_name)\n",
    "#         os.makedirs(self.dir_name, exist_ok=True)\n",
    "#         if not os.path.exists(file_path):\n",
    "#             urllib.request.urlretrieve(url, gz_file_path) \n",
    "#         with gzip.open(gz_file_path) as data:\n",
    "#             with open(file_path, 'wb') as out:\n",
    "#                 out.write(data.read())\n",
    "#         return file_path\n",
    "        \n",
    "#     def get_all_data(self):\n",
    "#         self.train_imgs_fn = self.get_data(self.url_train_imgs)\n",
    "#         self.train_labels_fn = self.get_data(self.url_train_labels)\n",
    "#         self.test_imgs_fn = self.get_data(self.url_test_imgs)\n",
    "#         self.test_labels_fn = self.get_data(self.url_test_labels)\n",
    "#         return self\n",
    "    \n",
    "#     def load_train(self):\n",
    "#         X, y = loadlocal_mnist(\n",
    "#             images_path=self.train_imgs_fn, \n",
    "#             labels_path=self.train_labels_fn)\n",
    "#         return X, y\n",
    "    \n",
    "#     def load_test(self):\n",
    "#         X, y = loadlocal_mnist(\n",
    "#             images_path=self.test_imgs_fn, \n",
    "#             labels_path=self.test_labels_fn)\n",
    "#         return X, y\n",
    "    \n",
    "#     def _split(self, X, y, test_size):\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(\n",
    "#             X, y, test_size=test_size, random_state=666)\n",
    "#         return X_train, X_test, y_train, y_test\n",
    "            \n",
    "#     def train_split(self, test_size):\n",
    "#         X, y = self.load_train()\n",
    "#         X_train, X_test, y_train, y_test = self._split(X, y, test_size)\n",
    "#         return X_train, X_test, y_train, y_test\n",
    "\n",
    "#     def standard_split(self):\n",
    "#         X_train, y_train = self.load_train()\n",
    "#         X_test, y_test = self.load_test()\n",
    "#         return X_train, X_test, y_train, y_test\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "num_classes = len(classes)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "data_loader = FashionMnistLoader().get_all_data()\n",
    "\n",
    "X_train_dev, X_test, y_train_dev, y_test = data_loader.standard_split()\n",
    "X_train_dev.shape, X_test.shape, len(y_train_dev), len(y_test)\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = data_loader.train_split(1/6)\n",
    "X_train.shape, X_dev.shape, len(y_train), len(y_dev)\n",
    "\n",
    "train_dataset = FashionMnist(X_train, y_train, transform=transform)\n",
    "dev_dataset = FashionMnist(X_dev, y_dev, transform=transform)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28 * 28\n",
    "\n",
    "class LinearNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = LinearNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs):\n",
    "    \n",
    "    loss_over_time = [] # to track the loss as the network trains\n",
    "    \n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "            # get the input images and their corresponding labels\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass to get outputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward pass to calculate the parameter gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # print loss statistics\n",
    "            # to convert loss into a scalar and add it to running_loss, we use .item()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if batch_i % 1000 == 999:    # print every 1000 batches\n",
    "                avg_loss = running_loss/1000\n",
    "                # record and print the avg loss over the 1000 batches\n",
    "                loss_over_time.append(avg_loss)\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, avg_loss))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    return loss_over_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1000, Avg. Loss: 0.44059368059411647\n",
      "Epoch: 1, Batch: 2000, Avg. Loss: 0.46890907338634136\n",
      "Epoch: 1, Batch: 3000, Avg. Loss: 0.4440670166835189\n",
      "Epoch: 2, Batch: 1000, Avg. Loss: 0.4460339671969414\n",
      "Epoch: 2, Batch: 2000, Avg. Loss: 0.44574086439237\n",
      "Epoch: 2, Batch: 3000, Avg. Loss: 0.4526415270343423\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2 # start small to see if your model works, initially\n",
    "model.train() # put model in train mode (important with, for example, dropouts, batch normalizations...)\n",
    "\n",
    "training_loss = train(n_epochs) # call train and record the loss over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.501134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize tensor and lists to monitor test loss and accuracy\n",
    "test_loss = torch.zeros(1)\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "# set the module to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "for batch_i, data in enumerate(test_loader):\n",
    "    \n",
    "    # get the input images and their corresponding labels\n",
    "    inputs, labels = data\n",
    "    # forward pass to get outputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "            \n",
    "    # update average test loss \n",
    "    test_loss = test_loss + ((torch.ones(1) / (batch_i + 1)) * (loss.data - test_loss))\n",
    "    \n",
    "    # get the predicted class from the maximum value in the output-list of class scores\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    # this creates a `correct` Tensor that holds the number of correctly classified images in a batch\n",
    "    correct = np.squeeze(predicted.eq(labels.data.view_as(predicted)))\n",
    "    \n",
    "    # calculate test accuracy for *each* object class\n",
    "    # we get the scalar value of correct items for a class, by calling `correct[i].item()`\n",
    "    for i in range(batch_size):\n",
    "        label = labels.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of T-shirt/top: 82% (855/1034)\n",
      "Test Accuracy of Trouser: 96% (966/1005)\n",
      "Test Accuracy of Pullover: 73% (739/1001)\n",
      "Test Accuracy of Dress: 80% (787/979)\n",
      "Test Accuracy of  Coat: 90% (872/959)\n",
      "Test Accuracy of Sandal: 85% (855/1004)\n",
      "Test Accuracy of Shirt: 43% (443/1007)\n",
      "Test Accuracy of Sneaker: 97% (983/1009)\n",
      "Test Accuracy of   Bag: 89% (915/1027)\n",
      "Test Accuracy of Ankle boot: 87% (856/975)\n",
      "\n",
      "Test Accuracy (Overall): 82% (8271/10000)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "        \n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "The following abstract class implements a generic regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Regularizer(object):\n",
    "    \"\"\"\n",
    "    Parent class of Regularizers\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def regularized_param(self, param_weights, reg_loss_function):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def regularized_all_param(self, reg_loss_function):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 Regularizer can be implemented as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Regularizer(_Regularizer):\n",
    "    \"\"\"\n",
    "    L1 regularized loss\n",
    "    \"\"\"\n",
    "    def __init__(self, model, lambda_reg=0):\n",
    "        super().__init__(model=model)\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def regularized_param(self, param_weights, reg_loss_function):\n",
    "        reg_loss_function += self.lambda_reg * self.__add_l1(var=param_weights)\n",
    "        return reg_loss_function\n",
    "\n",
    "    def regularized_all_param(self, reg_loss_function):\n",
    "        for model_param_name, model_param_value in self.model.named_parameters():\n",
    "            if model_param_name.endswith('weight'):\n",
    "                reg_loss_function += self.lambda_reg * self.__add_l1(var=model_param_value)\n",
    "        return reg_loss_function\n",
    "\n",
    "    @staticmethod\n",
    "    def __add_l1(var):\n",
    "        return var.abs().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularizer adds the regularization to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_loss = L1Regularizer(model).regularized_all_param(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
