{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oriol Domingo Roig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = pd.read_csv(\"data/jigsaw-toxic-comment-classification-challenge/train.csv\")\n",
    "comments_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was done due to memory and cpu limitation of my laptop. However, it was only necessary when adding an extra layer to the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39891"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = int(len(comments_df)/4)\n",
    "comments_df = comments_df.iloc[1:n]\n",
    "len(comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>Hello \\n\\nI guess the reqeust for the lion vs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36054</th>\n",
       "      <td>Also, you haven't said what information was de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comment_text\n",
       "3949   Hello \\n\\nI guess the reqeust for the lion vs ...\n",
       "36054  Also, you haven't said what information was de..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_colnames = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(comments_df[['comment_text']], comments_df[label_colnames], random_state=667)\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = None # Free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class `TextPreprocessor` has been modified to allow processing '.' and ',' so then they can be *tokenized*. Moreover, `balanced_partition` allows increasing the amount of positive classes by setting a minimum frequency that each label should appear in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z .,]')\n",
    "STEMMER = SnowballStemmer('english')\n",
    "\n",
    "class TextPreprocessor:\n",
    "        \n",
    "    def transfrom_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(BAD_SYMBOLS_RE, \" \", text) # process bad symbols\n",
    "        # text = \" \".join([STEMMER.stem(word) for word in text.split()])\n",
    "        return text\n",
    "    \n",
    "    def transform(self, series):\n",
    "        return series.apply(lambda text: self.transfrom_text(text))\n",
    "    \n",
    "    def labels_to_text(self, text, extra_labels):\n",
    "        for idx in extra_labels.index:\n",
    "            aux = text.loc[[idx]]\n",
    "            text = text.append(aux.iloc[[0]])\n",
    "        return text    \n",
    "    \n",
    "    def over_sampling(self,text,labels,extra_positive):\n",
    "        extra_labels = pd.DataFrame()\n",
    "        for idx in range(len(extra_positive)):\n",
    "            target = extra_positive[idx]\n",
    "            if idx == 0:\n",
    "                extra_labels = labels.groupby([target]).get_group(1)\n",
    "            else:\n",
    "                prev_target = extra_positive[idx-1]\n",
    "                extra_labels = extra_labels[extra_labels[target] == extra_labels[prev_target]]\n",
    "        \n",
    "        labels = labels.append(extra_labels)\n",
    "        text = self.labels_to_text(text,extra_labels)\n",
    "\n",
    "        return text,labels\n",
    "    \n",
    "    def balanced_partition(self,text,labels,min_freq):\n",
    "        while True :\n",
    "            labels_dist = labels.sum()/len(labels)\n",
    "            extra_positive = []\n",
    "            for label in labels_dist.keys():\n",
    "                if labels_dist[label] < min_freq:\n",
    "                    extra_positive.append(label) \n",
    "            if len(extra_positive) == 0: \n",
    "                return text,labels\n",
    "            else: \n",
    "                text,labels = self.over_sampling(text,labels,extra_positive)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            0.097232\n",
       "severe_toxic     0.010763\n",
       "obscene          0.052376\n",
       "threat           0.003443\n",
       "insult           0.049535\n",
       "identity_hate    0.008991\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sum()/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "X_train_trs = preprocessor.transform(X_train['comment_text'])\n",
    "X_train_preprocessed, y_train = preprocessor.balanced_partition(X_train_trs,y_train,0.02)\n",
    "X_test_preprocessed = preprocessor.transform(X_test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello \n",
      "\n",
      "I guess the reqeust for the lion vs tiger consenses I asked of Keith, will be  adressed to you:\n",
      "\n",
      "https://en.wikipedia.org/wiki/User_talk:Keithbob\n",
      "\n",
      "So if you may: \n",
      "\n",
      "Thanks: ^_^\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "hello   i guess the reqeust for the lion vs tiger consenses i asked of keith, will be  adressed to you   https   en.wikipedia.org wiki user talk keithbob  so if you may    thanks     \n"
     ]
    }
   ],
   "source": [
    "print(X_train[\"comment_text\"].iloc[0])\n",
    "print('---------------------------------------------------------------------------------------------------------------------')\n",
    "print(X_train_preprocessed.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(toxic            0.114023\n",
       " severe_toxic     0.029162\n",
       " obscene          0.070002\n",
       " threat           0.021978\n",
       " insult           0.067213\n",
       " identity_hate    0.027423\n",
       " dtype: float64, 30485, 30485)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sum()/len(y_train),len(X_train_preprocessed),len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trs = None # Free memory\n",
    "X_train = None     # Free memory\n",
    "X_test = None      # Free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `create_dicts` reduces vocabulary size by eliminating those words that appears less than *min_count* or that they are considered as *STOP_WORDS* in english. If '.' and ',' are found, they are mapped to `<dot>` and `<coma>` respectively. It was necessary to handle '.' and ',', this is why in `Tokenizer` I have add a convert function that allows to correctly map dots and comas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = get_stop_words('en')\n",
    "\n",
    "def create_dicts(text, min_count = 1):\n",
    "    \n",
    "    word_dict = {}\n",
    "    words = text.split()\n",
    "   \n",
    "    for word in words:\n",
    "        word_dict[word] = word_dict.get(word,0) + 1\n",
    "        \n",
    "    frequent_words = []\n",
    "    \n",
    "    for key, value in word_dict.items():\n",
    "        if value >= min_count and key not in STOP_WORDS:\n",
    "            if key == '.': key = '<dot>'\n",
    "            elif key == ',': key = '<coma>'\n",
    "            frequent_words.append(key)\n",
    "            \n",
    "    word_list = [\"<UNK>\", \"<PAD>\"] + sorted(frequent_words)\n",
    "    \n",
    "    word2idx = {word_list[idx]: idx for idx in range(len(word_list))}\n",
    "    idx2word = {idx: word_list[idx] for idx in range(len(word_list))}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = None\n",
    "        self.idx2word = None\n",
    "        \n",
    "    def fit(self, X,min_count):\n",
    "        text = \" \".join(X)\n",
    "        self.word2idx, self.idx2word = create_dicts(text,min_count)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [self.transform_line(line) for line in X]\n",
    "        \n",
    "    def transform_line(self, line):\n",
    "        return [self.word2idx.get(self.convert(word), 0) for word in line.split()]\n",
    "    \n",
    "    def convert(self,word):\n",
    "            if word == '.': word = '<dot>'\n",
    "            elif word == ',': word = '<coma>'\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit(X_train_preprocessed,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = tokenizer.transform(X_train_preprocessed)\n",
    "X_test_tokenized = tokenizer.transform(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = None  # Free memory\n",
    "X_test_preprocessed = None   # Free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutter:\n",
    "\n",
    "    def __init__(self, size=150):\n",
    "        self.size = size\n",
    "        \n",
    "    def transform(self, X):\n",
    "        new_X = []\n",
    "        for line in X:\n",
    "            new_line = line[:self.size]\n",
    "            new_line = new_line + [1] * (self.size - len(new_line))\n",
    "            new_X.append(new_line)\n",
    "        return new_X   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutter = Cutter()\n",
    "X_train_cutted = cutter.transform(X_train_tokenized)\n",
    "X_test_cutted = cutter.transform(X_test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = None   # Free memory\n",
    "X_test_tokenized = None    # Free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.from_numpy(y_train.values)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.tensor(X_train_cutted), torch.from_numpy(y_train.values).float())\n",
    "test_data = TensorDataset(torch.tensor(X_test_cutted), torch.from_numpy(y_test.values).float())\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, dict_size, output_size, embedding_dim, hidden_dim, hidden_inter_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(dict_size, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_inter_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_inter_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm1(embeded)\n",
    "        #lstm_out = lstm_out[:, -1] \n",
    "        lstm_outer,_ = self.lstm2(lstm_out)\n",
    "        lstm_outer = lstm_outer[:,-1]\n",
    "        logits = self.fc(lstm_outer)\n",
    "        out = self.sigmoid(logits)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(tokenizer.word2idx)\n",
    "output_size = len(label_colnames)\n",
    "embedding_dim = 3\n",
    "hidden_dim = 4\n",
    "hidden_inter_dim = 3\n",
    "\n",
    "lstm_model = LSTMModel(dict_size, output_size, embedding_dim, hidden_dim,hidden_inter_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.5976, 0.4501, 0.4720, 0.5966, 0.4606, 0.3849],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925],\n",
       "        [0.6134, 0.4585, 0.4685, 0.5976, 0.4615, 0.3925]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "input_data, labels = dataiter.next()\n",
    "lstm_model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 300, Avg. Loss: 0.21438441023230553\n",
      "Epoch: 1, Batch: 600, Avg. Loss: 0.20713056550671657\n",
      "Epoch: 1, Batch: 900, Avg. Loss: 0.19493156573424736\n",
      "Epoch: 2, Batch: 300, Avg. Loss: 0.19825190457204978\n",
      "Epoch: 2, Batch: 600, Avg. Loss: 0.1915242950618267\n",
      "Epoch: 2, Batch: 900, Avg. Loss: 0.1985955570762356\n",
      "Epoch: 3, Batch: 300, Avg. Loss: 0.2075862596432368\n",
      "Epoch: 3, Batch: 600, Avg. Loss: 0.20558393610020478\n",
      "Epoch: 3, Batch: 900, Avg. Loss: 0.19684397413084903\n",
      "Epoch: 4, Batch: 300, Avg. Loss: 0.1948415905609727\n",
      "Epoch: 4, Batch: 600, Avg. Loss: 0.19865096479654312\n",
      "Epoch: 4, Batch: 900, Avg. Loss: 0.20518127150833607\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 4\n",
    "print_every = 300\n",
    "\n",
    "lstm_model.train()\n",
    "optimizer.train = True\n",
    "\n",
    "loss_over_time = [] # to track the loss as the network trains\n",
    "    \n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_i, (input_data, labels) in enumerate(train_loader):\n",
    "        # Zero gradients (just in case)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass, calculate predictions\n",
    "        output = lstm_model(input_data) \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, labels)\n",
    "        ## Backward propagation\n",
    "        loss.backward()\n",
    "        ## Upade weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print loss statistics\n",
    "        # to convert loss into a scalar and add it to running_loss, we use .item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if batch_i % print_every ==  print_every - 1:    # print everyx batches (\n",
    "                avg_loss = running_loss/print_every\n",
    "                # record and print the avg loss over the 100 batches\n",
    "                loss_over_time.append(avg_loss)\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, avg_loss))\n",
    "                running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_over_time)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following list there is a record of the results obtained playing with some parameters as well as adding an extra layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Type : LSTM-MLP   ; Embeding: 3 ; Hidden Dim : 4   ; Epochs : 1  ; LR : 0.005 ; Avg Loss : 0.167 \n",
    "2. Type : LSTM-MLP   ; Embeding: 4 ; Hidden Dim : 4   ; Epochs : 1  ; LR : 0.005 ; Avg Loss : 0.173\n",
    "3. Type : LSTM-MLP   ; Embeding: 3 ; Hidden Dim : 2   ; Epochs : 1  ; LR : 0.005 ; Avg Loss : 0.171\n",
    "4. Type : LSTM-MLP   ; Embeding: 3 ; Hidden Dim : 3   ; Epochs : 1  ; LR : 0.005 ; Avg Loss : 0.174\n",
    "5. Type : 2xLSTM-MLP ; Embeding: 3 ; Hidden Dim : 4-3 ; Epochs : 4  ; LR : 0.05  ; Avg Loss : 0.116\n",
    "6. Type : 2xLSTM-MLP ; Embeding: 3 ; Hidden Dim : 3-4 ; Epochs : 4  ; LR : 0.05  ; Avg Loss : 0.201\n",
    "7. Type : 2xLSTM-MLP ; Embeding: 2 ; Hidden Dim : 4-3 ; Epochs : 4  ; LR : 0.05  ; Avg Loss : 0.205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(y, y_hat, y_proba):\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y, y_hat),\n",
    "        \"Precision\": precision_score(y, y_hat),\n",
    "        \"Recall\": recall_score(y, y_hat),\n",
    "        \"F1-score\": f1_score(y, y_hat),\n",
    "        \"AUC\": roc_auc_score(y, y_proba),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.000015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oriol/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/oriol/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/oriol/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/oriol/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/oriol/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/oriol/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/oriol/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/oriol/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/oriol/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/oriol/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/oriol/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/oriol/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_classes = len(label_colnames)\n",
    "# initialize tensor and lists to monitor test loss and accuracy\n",
    "test_loss = torch.zeros(1)\n",
    "class_correct = list(0. for i in range(num_classes))\n",
    "class_total = list(0. for i in range(num_classes))\n",
    "\n",
    "# set the module to evaluation mode\n",
    "lstm_model.eval()\n",
    "\n",
    "# get the input images and their corresponding labels\n",
    "inputs, labels = test_loader.dataset.tensors\n",
    "\n",
    "# forward pass to get outputs\n",
    "outputs = lstm_model(inputs)\n",
    "\n",
    "# calculate the loss\n",
    "loss = criterion(outputs, labels)\n",
    "\n",
    "# update average test loss \n",
    "test_loss = test_loss + ((torch.ones(1) / (len(labels) + 1)) * (loss.data - test_loss))\n",
    "\n",
    "# get the predicted class from the maximum value in the output-list of class scores\n",
    "metrics = {}\n",
    "for j in range(num_classes):\n",
    "    # compare predictions to true label\n",
    "    predicted_class = np.round(outputs.data[:,j])\n",
    "    labels_class = labels.data[:,j]\n",
    "    class_total[j] = len(labels)\n",
    "    class_correct[j] = (labels_class==predicted_class).sum()\n",
    "    metrics[label_colnames[j]] = evaluate_classification(labels_class, predicted_class, outputs.data[:,j])\n",
    "    #(predicted_class == labels_class).sum()\n",
    "              \n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insult\n",
      "{'Accuracy': 0.9488619272034493, 'AUC': 0.49832381224707994, 'Recall': 0.0, 'F1-score': 0.0, 'Precision': 0.0}\n",
      "identity_hate\n",
      "{'Accuracy': 0.9908753634813998, 'AUC': 0.5032120783486904, 'Recall': 0.0, 'F1-score': 0.0, 'Precision': 0.0}\n",
      "severe_toxic\n",
      "{'Accuracy': 0.9896721147097163, 'AUC': 0.5079888059334454, 'Recall': 0.0, 'F1-score': 0.0, 'Precision': 0.0}\n",
      "obscene\n",
      "{'Accuracy': 0.9444500150406097, 'AUC': 0.5033525445725151, 'Recall': 0.0, 'F1-score': 0.0, 'Precision': 0.0}\n",
      "toxic\n",
      "{'Accuracy': 0.8971222300210568, 'AUC': 0.5019210486009119, 'Recall': 0.0, 'F1-score': 0.0, 'Precision': 0.0}\n",
      "threat\n",
      "{'Accuracy': 0.9968916073398175, 'AUC': 0.5031683765841883, 'Recall': 0.0, 'F1-score': 0.0, 'Precision': 0.0}\n"
     ]
    }
   ],
   "source": [
    "for label in metrics:\n",
    "    print(label)\n",
    "    print(metrics[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following list there is a record of the performances metrics obtained in the test set regarding methods described above. Each reference number corresponds to the model assigned with the same number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Test Loss : 0.000004 ; Accuracy : 0.9608 ; Precision : 0.433\n",
    "2. Test Loss : 0.000004 ; Accuracy : 0.9605 ; Precision : 0.361\n",
    "3. Test Loss : 0.000004 ; Accuracy : 0.9623 ; Precision : 0.0\n",
    "4. Test Loss : 0.000004 ; Accuracy : 0.9630 ; Precision : 0.291\n",
    "5. Test Loss : 0.000013 ; Accuracy : 0.9615 ; Precision : 0.375\n",
    "6. Test Loss : 0.000015 ; Accuracy : 0.9597 ; Precision : 0.223\n",
    "7. Test Loss : 0.000015 ; Accuracy : 0.9571 ; Precision : 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
